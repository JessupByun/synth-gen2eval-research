import os
import numpy as np
import pandas as pd
from tabpfn import TabPFNClassifier
from sklearn.preprocessing import KBinsDiscretizer

def generate_synthetic_data(df, n_bins=10):
    """
    Generates synthetic data using TabPFN on the provided DataFrame.
    The rightmost column is forced to be the target.
    
    Features (all columns except the rightmost):
      - Numeric features are quantile binned.
      - Categorical features are converted to integer codes.
    Target (the rightmost column):
      - If numeric, it is binned with KBinsDiscretizer.
      - If non-numeric, it is converted to category codes (with a mapping to decode later).
    
    The TabPFNClassifier is then trained on the processed features and target.
    Synthetic features are generated by sampling within the binned ranges and then “inverted”
    back to continuous values for numeric features (or decoded for categorical ones).
    
    Returns a synthetic DataFrame with the same number of rows as the original, with the target as the rightmost column.
    """
    # Force the rightmost column to be the target.
    target_col = df.columns[-1]
    feature_cols = df.columns[:-1]
    
    # Separate features and target.
    features = df[feature_cols].copy()
    target = df[target_col].copy()

    # --- Process features ---
    # Identify numeric and categorical features.
    numeric_features = features.select_dtypes(include=[np.number]).columns.tolist()
    categorical_features = features.select_dtypes(exclude=[np.number]).columns.tolist()

    # For numeric features, apply quantile binning.
    kbins_dict = {}
    binned_features = features.copy()
    for col in numeric_features:
        kbins = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy="quantile", subsample=None)
        binned_vals = kbins.fit_transform(features[[col]]).astype(int).flatten()
        binned_features[col] = binned_vals
        kbins_dict[col] = kbins

    # For categorical features, convert to codes.
    for col in categorical_features:
        binned_features[col] = features[col].astype('category').cat.codes

    # --- Process target ---
    # If the target is numeric, bin it; if categorical, encode it.
    if pd.api.types.is_numeric_dtype(target):
        kbins_target = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy="quantile", subsample=None)
        target_binned = kbins_target.fit_transform(target.values.reshape(-1, 1)).astype(int).flatten()
        target_processed = target_binned
        target_mapping = None  # No mapping needed for numeric target.
    else:
        target_cat = target.astype('category')
        target_processed = target_cat.cat.codes.values  # Numeric codes for target.
        target_mapping = dict(enumerate(target_cat.cat.categories))
        kbins_target = None

    # --- Prepare training data ---
    X_train = binned_features  # Features after processing.
    y_train = pd.Series(target_processed, index=target.index)

    # --- Train TabPFN ---
    model = TabPFNClassifier(device='cpu')
    model.fit(X_train, y_train)

    # --- Generate synthetic features ---
    n_synth = df.shape[0]
    synthetic_X = np.empty((n_synth, X_train.shape[1]), dtype=int)
    for i, col in enumerate(X_train.columns):
        col_min = X_train[col].min()
        col_max = X_train[col].max()
        synthetic_X[:, i] = np.random.randint(col_min, col_max + 1, size=n_synth)
    synthetic_features = pd.DataFrame(synthetic_X, columns=X_train.columns)

    # --- Predict synthetic target ---
    # Note: synthetic_features is a DataFrame with column names, so the warning should be avoided.
    synthetic_y_pred = model.predict(synthetic_features)

    # --- Invert binning for numeric features in synthetic data ---
    synthetic_features_cont = synthetic_features.copy()
    for col in numeric_features:
        edges = kbins_dict[col].bin_edges_[0]
        n_edges = len(edges) - 1
        def sample_in_bin(bin_idx):
            if 0 <= bin_idx < n_edges:
                low, high = edges[bin_idx], edges[bin_idx+1]
                return np.random.uniform(low, high)
            else:
                return np.nan
        synthetic_features_cont[col] = synthetic_features_cont[col].apply(sample_in_bin)

    # For categorical features, decode the integer codes back to categories.
    synthetic_features_decoded = synthetic_features_cont.copy()
    for col in categorical_features:
        original_categories = features[col].astype('category').cat.categories
        def map_cat_code(x):
            if 0 <= x < len(original_categories):
                return original_categories[x]
            else:
                return np.nan
        synthetic_features_decoded[col] = synthetic_features_decoded[col].apply(map_cat_code)

    # --- Process synthetic target ---
    if pd.api.types.is_numeric_dtype(target):
        # Invert binning for numeric target.
        target_edges = kbins_target.bin_edges_[0]
        n_target_edges = len(target_edges) - 1
        def sample_target_in_bin(bin_idx):
            if 0 <= bin_idx < n_target_edges:
                low, high = target_edges[bin_idx], target_edges[bin_idx+1]
                return np.random.uniform(low, high)
            else:
                return np.nan
        synthetic_target = [sample_target_in_bin(b) for b in synthetic_y_pred]
    else:
        # For categorical target, simply decode using target_mapping.
        synthetic_target = [target_mapping.get(code, np.nan) for code in synthetic_y_pred]

    # --- Reassemble synthetic data ---
    synthetic_df = synthetic_features_decoded.copy()
    synthetic_df[target_col] = synthetic_target

    # Optionally, reorder columns to match original order.
    synthetic_df = synthetic_df[list(feature_cols) + [target_col]]

    return synthetic_df

def process_csv_file(input_csv, output_csv, n_bins=10, batch_size=200):
    """
    Loads and shuffles a CSV with seed=42. If <= batch_size rows, processes in one go.
    Otherwise, partitions the DataFrame into up to batch_size rows per chunk,
    generates synthetic data for each chunk, then concatenates all chunks.
    Saves the final result to 'output_csv'.
    """
    df = pd.read_csv(input_csv)
    # Shuffle entire DataFrame with seed=42.
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    n_rows = df.shape[0]

    if n_rows <= batch_size:
        synthetic_df = generate_synthetic_data(df, n_bins=n_bins)
    else:
        synthetic_batches = []
        # Partition the DataFrame into consecutive batches.
        for start in range(0, n_rows, batch_size):
            batch_df = df.iloc[start:start+batch_size]
            synthetic_batch = generate_synthetic_data(batch_df, n_bins=n_bins)
            synthetic_batches.append(synthetic_batch)
        synthetic_df = pd.concat(synthetic_batches, ignore_index=True)

    if synthetic_df.shape[0] != n_rows:
        print(f"[WARNING] Synthetic row count ({synthetic_df.shape[0]}) differs from original ({n_rows}).")

    synthetic_df.to_csv(output_csv, index=False)
    print(f"[INFO] Synthetic data saved to {output_csv}")

def process_dataset(dataset_name, generator_name="tabpfn", n_bins=10, batch_size=200):
    """
    Processes all CSV files found in:
        LTM_data/LTM_real_data/{dataset_name}/train/
    Outputs synthetic CSVs under:
        LTM_data/LTM_synthetic_data/synth_{dataset_name}/
    Each output is named:
        {original_csv_filename}_{generator_name}_default_0.csv
    """
    real_data_path = os.path.join("LTM_data", "LTM_real_data", dataset_name)
    train_folder = os.path.join(real_data_path, "train")
    if not os.path.isdir(train_folder):
        print(f"[ERROR] Train folder not found: {train_folder}")
        return

    synthetic_folder = os.path.join("LTM_data", "LTM_synthetic_data", f"synth_{dataset_name}")
    os.makedirs(synthetic_folder, exist_ok=True)

    csv_files = [f for f in os.listdir(train_folder) if f.lower().endswith(".csv")]
    if not csv_files:
        print(f"[WARNING] No CSV files found in {train_folder}.")
        return

    for csv_file in csv_files:
        input_csv_path = os.path.join(train_folder, csv_file)
        base_name = os.path.splitext(csv_file)[0]
        output_csv_name = f"{base_name}_{generator_name}_default_0.csv"
        output_csv_path = os.path.join(synthetic_folder, output_csv_name)

        print(f"[INFO] Processing: {input_csv_path} -> {output_csv_path}")
        process_csv_file(input_csv_path, output_csv_path, n_bins=n_bins, batch_size=batch_size)

if __name__ == "__main__":
    # You can hard-code the dataset name or prompt the user for the full relative path.
    dataset_name = "airfoil-self-noise"  # Adjust this as needed.
    # For prompting:
    # dataset_name = input("Enter dataset folder name (under LTM_data/LTM_real_data): ")

    process_dataset(dataset_name, generator_name="tabpfn", n_bins=10, batch_size=200)
