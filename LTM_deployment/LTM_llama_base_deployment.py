import os
import numpy as np
import pandas as pd
import requests
from io import StringIO

# --- Configuration for Groq API ---
GROQ_API_URL = "https://api.groq.com/v1/inference"  # Replace with the actual endpoint

# Access the Groq API key (should be securely stored in the .env file (not provided, must be generated by user))
api_key = os.getenv("GROQ_API_KEY")

# For "llama-3.3-70B-versatile" or similar models, you may set max_tokens up to the model's limit (e.g., 32000).
MAX_COMPLETION_TOKENS = 32000  

# --- Prompt Template ---
prompt_template = (
    "Below is a CSV representation of the dataset '{dataset_name}'.\n"
    "Columns: {columns}\n"
    "Here are a few sample rows:\n{sample_rows}\n\n"
    "Please generate synthetic data for this dataset. "
    "Your entire response must be valid JSON with a key 'synthetic_data' that contains the CSV of the synthetic data. "
    "Do not include any additional text."
)

def call_groq_api(prompt, max_tokens=MAX_COMPLETION_TOKENS):
    """
    Calls the Groq API with the given prompt.
    Returns the response JSON.
    """
    payload = {
         "prompt": prompt,
         "max_tokens": max_tokens,
         "temperature": 0.7,  # Adjust as needed
         "forced_json_output": True  # Instruct the model to output valid JSON
    }
    headers = {
         "Authorization": f"Bearer {API_KEY}",
         "Content-Type": "application/json"
    }
    response = requests.post(GROQ_API_URL, json=payload, headers=headers)
    response.raise_for_status()  # Raise an HTTPError if the request failed
    return response.json()

def generate_synthetic_data_llama(df, dataset_name):
    """
    Generates synthetic data using the LLM via the Groq API.
    - Takes a small sample of rows to form the prompt.
    - Calls the LLM, instructing it to return a JSON object containing synthetic CSV data.
    - Logs token usage from the response (if provided).
    - Returns a synthetic DataFrame parsed from the 'synthetic_data' field in the JSON response.
    """
    # Build prompt parameters.
    columns = ", ".join(df.columns)
    # Use a small sample of rows to avoid large prompts.
    sample_rows = df.head(5).to_csv(index=False)

    prompt = prompt_template.format(
        dataset_name=dataset_name,
        columns=columns,
        sample_rows=sample_rows
    )
    
    # Call the Groq API.
    response_json = call_groq_api(prompt)

    # Extract and log token usage info (if provided by the API).
    usage = response_json.get("usage", {})
    prompt_tokens = usage.get("prompt_tokens", "N/A")
    completion_tokens = usage.get("completion_tokens", "N/A")
    total_tokens = usage.get("total_tokens", "N/A")
    print(f"[INFO] Token usage: prompt={prompt_tokens}, completion={completion_tokens}, total={total_tokens}")
    
    # Extract the synthetic data CSV string.
    synthetic_csv_str = response_json.get("synthetic_data", "")
    if not synthetic_csv_str:
        raise ValueError("No 'synthetic_data' key in API response!")
    
    # Convert CSV string to DataFrame.
    synthetic_df = pd.read_csv(StringIO(synthetic_csv_str))
    return synthetic_df

def process_csv_file_llama(input_csv, output_csv, dataset_name, batch_size=200):
    """
    Processes a single CSV file:
    - Loads and shuffles the CSV (seed=42).
    - If <= batch_size rows, processes in one go.
    - Otherwise, partitions the DataFrame into chunks of up to batch_size rows,
      calls the LLM for each chunk, and concatenates outputs.
    - Saves the final synthetic DataFrame to 'output_csv'.
    """
    df = pd.read_csv(input_csv)
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    n_rows = df.shape[0]
    
    if n_rows <= batch_size:
        synthetic_df = generate_synthetic_data_llama(df, dataset_name)
    else:
        synthetic_batches = []
        for start in range(0, n_rows, batch_size):
            batch_df = df.iloc[start:start+batch_size]
            synthetic_batch = generate_synthetic_data_llama(batch_df, dataset_name)
            synthetic_batches.append(synthetic_batch)
        synthetic_df = pd.concat(synthetic_batches, ignore_index=True)
    
    if synthetic_df.shape[0] != n_rows:
        print(f"[WARNING] Synthetic row count ({synthetic_df.shape[0]}) differs from original ({n_rows}).")
    
    synthetic_df.to_csv(output_csv, index=False)
    print(f"[INFO] Synthetic data saved to {output_csv}")

def process_dataset_llama(dataset_name, generator_name="llama", batch_size=200):
    """
    Processes all CSV files in:
      LTM_data/LTM_real_data/{dataset_name}/train/
    Synthetic CSVs are saved under:
      LTM_data/LTM_synthetic_data/LTM_llama_synthetic_data/synth_{dataset_name}/
    Each output is named:
      {original_csv_filename}_{generator_name}_default_0.csv
    """
    real_data_path = os.path.join("LTM_data", "LTM_real_data", dataset_name)
    train_folder = os.path.join(real_data_path, "train")
    if not os.path.isdir(train_folder):
        print(f"[ERROR] Train folder not found: {train_folder}")
        return
    
    # Updated folder path per your requirement
    synthetic_folder = os.path.join("LTM_data", "LTM_synthetic_data", "LTM_llama_synthetic_data", f"synth_{dataset_name}")
    os.makedirs(synthetic_folder, exist_ok=True)
    
    csv_files = [f for f in os.listdir(train_folder) if f.lower().endswith(".csv")]
    if not csv_files:
        print(f"[WARNING] No CSV files found in {train_folder}.")
        return
    
    for csv_file in csv_files:
        input_csv_path = os.path.join(train_folder, csv_file)
        base_name = os.path.splitext(csv_file)[0]
        output_csv_name = f"{base_name}_{generator_name}_default_0.csv"
        output_csv_path = os.path.join(synthetic_folder, output_csv_name)
        
        print(f"[INFO] Processing: {input_csv_path} -> {output_csv_path}")
        process_csv_file_llama(input_csv_path, output_csv_path, dataset_name, batch_size=batch_size)

if __name__ == "__main__":
    dataset_name = "airfoil-self-noise"  # Adjust as needed
    process_dataset_llama(dataset_name, generator_name="llama", batch_size=200)