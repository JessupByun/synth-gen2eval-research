import os
import pandas as pd
from sklearn.model_selection import train_test_split
from dotenv import load_dotenv
from groq import Groq

# Load the real data
data_csv = "data/real_data/churn/Churn_Modelling.csv"
data = pd.read_csv(data_csv)

# Split the data into train and test sets
train_data, test_data = train_test_split(data, test_size=0.8, random_state=42)
test_data.to_csv('data/real_data/churn/churn_modeling_test.csv', index=False)
train_data.to_csv('data/real_data/churn/churn_modeling_train.csv', index=False) # Will include the entire training data, which will then be sampled in n sample sizes below.

# Define the n sample size of train_data
train_data = train_data.sample(200, random_state = 42)

# Define temperature parameter for model (controls randomness and diversity, as temp -> 0, model becomes more deterministic and repetitive)
temperature = 1

# Load environment variables from the .env file
load_dotenv()

# Access the Groq API key (should be securely stored in the .env file (not provided, must be generated by user))
api_key = os.getenv("GROQ_API_KEY")

# Instantiate the Groq client with API key
client = Groq(api_key=api_key)

# List of model ID names that will be deployed. Visit groq API documentation for more models
model_names = ["llama-3.3-70b-versatile"]

# This prompt structure is adapted from the prompt example B.5. from the research paper: "Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in low-data regimes" (Seedatk, Huynh, et al.) https://arxiv.org/pdf/2312.12112 
# The template is currently adapted to the 'insurance.csv' dataset (referenced in README.md)
prompt_template_baseline = """
System role: You are a tabular synthetic data generation model.

You are a synthetic data generator.
Your goal is to produce data which mirrors the given examples in causal structure and feature and label distributions but also produce as diverse samples as possible.

I will give you real examples first.

Context: Leverage your knowledge about customer churn, banking, and demographics to generate 200 realistic but diverse samples. 
Output the data in a csv format where I can directly copy and paste into a csv.

Example data: {data}

The output should use the following schema:

"RowNumber": integer // feature column for row number
"CustomerId": integer // feature column for the customer ID
"Surname": string // feature column for the surname of the customer
"CreditScore": integer // feature column for the customer's credit score
"Geography": string // feature column for geography, possible values: "France", "Germany", "Spain"
"Gender": string // feature column for gender, possible values: "Male", "Female"
"Age": integer // feature column for the customer's age
"Tenure": integer // feature column for the number of years the customer has been with the bank
"Balance": float // feature column for the customer's bank balance
"NumOfProducts": integer // feature column for the number of products the customer has
"HasCrCard": integer // feature column indicating if the customer has a credit card (1 for yes, 0 for no)
"IsActiveMember": integer // feature column indicating if the customer is an active member (1 for yes, 0 for no)
"EstimatedSalary": float // feature column for the customer's estimated salary
"Exited": integer // target label column, 1 for customers who exited the bank, 0 for those who stayed

DO NOT COPY THE EXAMPLES but generate realistic but new and diverse samples which have the correct label conditioned on the features.
"""

prompt_template_advanced = """
System role: You are a tabular synthetic data generation model.

You are a synthetic data generator.
Your goal is to produce data which mirrors the given examples in causal structure and feature and label distributions but also produce as diverse samples as possible.

I will give you real examples first.

Context: Leverage your knowledge about customer churn, banking, and demographics to generate 200 realistic but diverse samples. 
Output the data in a csv format where I can directly copy and paste into a csv.

Example data: {data}

The output should use the following schema:

"RowNumber": integer // feature column for row number
"CustomerId": integer // feature column for the customer ID
"Surname": string // feature column for the surname of the customer
"CreditScore": integer // feature column for the customer's credit score
"Geography": string // feature column for geography, possible values: "France", "Germany", "Spain"
"Gender": string // feature column for gender, possible values: "Male", "Female"
"Age": integer // feature column for the customer's age
"Tenure": integer // feature column for the number of years the customer has been with the bank
"Balance": float // feature column for the customer's bank balance
"NumOfProducts": integer // feature column for the number of products the customer has
"HasCrCard": integer // feature column indicating if the customer has a credit card (1 for yes, 0 for no)
"IsActiveMember": integer // feature column indicating if the customer is an active member (1 for yes, 0 for no)
"EstimatedSalary": float // feature column for the customer's estimated salary
"Exited": integer // target label column, 1 for customers who exited the bank, 0 for those who stayed

Here are detailed summary stats that you should also use:

,count,unique,top,freq,mean,std,min,25%,50%,75%,max
RowNumber,10000.0,,,,5000.5,2886.8956799071675,1.0,2500.75,5000.5,7500.25,10000.0
CustomerId,10000.0,,,,15690940.5694,71936.1861227489,15565701.0,15628528.25,15690738.0,15753233.75,15815690.0
Surname,10000,2932,Smith,32,,,,,,,
CreditScore,10000.0,,,,650.5288,96.65329873613035,350.0,584.0,652.0,718.0,850.0
Geography,10000,3,France,5014,,,,,,,
Gender,10000,2,Male,5457,,,,,,,
Age,10000.0,,,,38.9218,10.487806451704609,18.0,32.0,37.0,44.0,92.0
Tenure,10000.0,,,,5.0128,2.8921743770496837,0.0,3.0,5.0,7.0,10.0
Balance,10000.0,,,,76485.889288,62397.405202385955,0.0,0.0,97198.54000000001,127644.24,250898.09
NumOfProducts,10000.0,,,,1.5302,0.5816543579989906,1.0,1.0,1.0,2.0,4.0
HasCrCard,10000.0,,,,0.7055,0.4558404644751333,0.0,0.0,1.0,1.0,1.0
IsActiveMember,10000.0,,,,0.5151,0.49979692845891893,0.0,0.0,1.0,1.0,1.0
EstimatedSalary,10000.0,,,,100090.239881,57510.49281769816,11.58,51002.11,100193.915,149388.2475,199992.48
Exited,10000.0,,,,0.2037,0.4027685839948609,0.0,0.0,0.0,0.0,1.0

DO NOT COPY THE EXAMPLES but generate realistic but new and diverse samples which have the correct label conditioned on the features.
"""

# Function to generate synthetic data using a model and prompt
def generate_synthetic_data(model_name, data):

    prompt = prompt_template_advanced.format(data = data)
    
    try:
        # Create a chat completion using the Groq API
        response = client.chat.completions.create(
            messages=[
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            model=model_name,
            #response_format={"type": "json_object"} Turn on for JSON beta mode
            temperature=temperature
        )
        
        # Print the full response for debugging
        print("Full Response:", response)
        
        generated_data = response.choices[0].message.content if response.choices else "No output"
        
        return generated_data
    except Exception as e:
        print(f"Error generating data with model {model_name}: {e}")
        return None

# Main function to run the process
def main():
    
    for model_name in model_names:
        print(f"Generating data with {model_name}...")
        
        # Generate synthetic data with n rows!
        data = generate_synthetic_data(model_name, train_data)

        print(f"Generated Data for {model_name}:\n{data}\n")

if __name__ == "__main__":
    main()